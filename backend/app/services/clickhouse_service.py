import os
import pandas as pd
import clickhouse_connect
import redis
import json
import hashlib
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
import dotenv

dotenv.load_dotenv()

# åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨ (ç”¨äºç”Ÿäº§ç¯å¢ƒæ ‡å‡†åŒ–æ—¥å¿—)
logger = logging.getLogger(__name__)

class ClickHouseService:
    def __init__(self):
        # ä¼˜å…ˆè¯»å–ç¯å¢ƒå˜é‡ (é»˜è®¤è¿æ¥ç”¨äº PnL åˆ†æç­‰é€šç”¨ä¸šåŠ¡)
        self.host = os.getenv("CLICKHOUSE_HOST", "dwsz2tfd9y.ap-northeast-1.aws.clickhouse.cloud")
        self.port = int(os.getenv("CLICKHOUSE_PORT", "8443"))
        self.username = os.getenv("CLICKHOUSE_USER", "default")
        
        # å¤„ç†å¯†ç  (å»é™¤å¯èƒ½å­˜åœ¨çš„é¦–å°¾ç©ºæ ¼)
        raw_password = os.getenv("CLICKHOUSE_PASSWORD")
        self.password = raw_password.strip() if raw_password else None
        
        self.database = os.getenv("CLICKHOUSE_DB", "Fxbo_Trades") 
        self.secure = True # å¼ºåˆ¶å¼€å¯ TLS

        # --- ç”Ÿäº§ç¯å¢ƒè¿æ¥é…ç½® (ç”¨äº IB æŠ¥è¡¨ç»„åˆ«ç­‰æ•æ„Ÿæ•°æ®æŸ¥è¯¢) ---
        self.prod_host = os.getenv("CLICKHOUSE_prod_HOST")
        self.prod_user = os.getenv("CLICKHOUSE_prod_USER")
        self.prod_pass = os.getenv("CLICKHOUSE_prod_PASSWORD")
        self.prod_db = "KCM_fxbackoffice"

        # --- å†…å­˜ç¼“å­˜ (ç”¨äº IB ç»„åˆ«åˆ—è¡¨ï¼Œæœ‰æ•ˆæœŸ 7 å¤©ï¼Œå‡å°‘ ClickHouse å‹åŠ›) ---
        self._group_cache = None
        self._cache_expiry = None

        # Redis åˆå§‹åŒ– (ç”¨äºä¸šåŠ¡æ•°æ®ç¼“å­˜)
        try:
            self.redis_client = redis.Redis(
                host=os.getenv("REDIS_HOST", "localhost"),
                port=int(os.getenv("REDIS_PORT", 6379)),
                db=0,
                decode_responses=True 
            )
        except Exception as e:
            logger.error(f"Redis initialization failed: {e}")
            self.redis_client = None

        # [å¯é€‰] å¯åŠ¨æ—¶å°è¯•è½»é‡è¿æ¥æµ‹è¯•
        try:
            with self.get_client() as client:
                client.command('SELECT 1')
            logger.info("âœ… ClickHouse Default Connection Established.")
        except Exception as e:
            logger.warning(f"âš ï¸ ClickHouse Connection Warning: {e}")

    def get_client(self, use_prod: bool = False):
        """
        è·å– ClickHouse å®¢æˆ·ç«¯è¿æ¥ã€‚
        :param use_prod: æ˜¯å¦ä½¿ç”¨ç”Ÿäº§ç¯å¢ƒè¿æ¥é…ç½®
        """
        if use_prod:
            # è¿™é‡Œçš„ host/user/password å¿…é¡»ä»ç”Ÿäº§ç¯å¢ƒå˜é‡è¯»å–
            return clickhouse_connect.get_client(
                host=self.prod_host,
                username=self.prod_user,
                password=self.prod_pass,
                database=self.prod_db,
                secure=True
            )
        return clickhouse_connect.get_client(
            host=self.host,
            port=self.port,
            username=self.username,
            password=self.password,
            database=self.database,
            secure=self.secure
        )

    def get_ib_groups(self) -> Dict[str, Any]:
        """
        è·å–æ‰€æœ‰ IB ç»„åˆ«åŠå…¶å¯¹åº”çš„ç”¨æˆ·æ•°é‡ã€‚
        å®ç°é€»è¾‘ï¼š
        1. ä¼˜å…ˆä»å†…å­˜ç¼“å­˜ä¸­è¯»å–æ•°æ®ã€‚
        2. å¦‚æœç¼“å­˜ä¸å­˜åœ¨æˆ–å·²è¶…è¿‡ 7 å¤©ï¼Œåˆ™ä»ç”Ÿäº§ ClickHouse æŸ¥è¯¢ã€‚
        3. é‡‡ç”¨æ‰¹é‡èšåˆæŸ¥è¯¢æ–¹å¼æé«˜æ€§èƒ½ã€‚
        """
        now = datetime.now()
        
        # 1. æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ (7 å¤©æœ‰æ•ˆæœŸ)
        if self._group_cache and self._cache_expiry and now < self._cache_expiry:
            logger.info("ğŸš€ Returning IB groups from memory cache.")
            return self._group_cache

        try:
            logger.info("ğŸ” Fetching IB groups from ClickHouse Prod...")
            with self.get_client(use_prod=True) as client:
                # ç¬¬ä¸€æ­¥ï¼šè·å–ç»„åˆ«åŸºç¡€ä¿¡æ¯ (categoryId=6 ä¸ºç»„åˆ«åˆ†ç±»)
                # æ³¨æ„ï¼šClickHouse è¡¨ååŒºåˆ†å¤§å°å†™ï¼Œä¸”åœ¨ç”Ÿäº§ç¯å¢ƒä¸‹å»ºè®®æ˜¾å¼å†™å‡ºè¡¨å
                tags_sql = 'SELECT id AS tag_id, tag AS tag_name FROM "fxbackoffice_tags" WHERE categoryId = 6'
                tags_result = client.query(tags_sql)
                tags_df = pd.DataFrame(tags_result.result_set, columns=tags_result.column_names)

                # ç¬¬äºŒæ­¥ï¼šè·å–ç»„åˆ«å…³è”çš„ç”¨æˆ·æ•° (å»é‡ç»Ÿè®¡)
                # è¿™ç§æ–¹å¼æ¯”å¯¹æ¯ä¸ªç»„åˆ«å•ç‹¬å‘ SQL æ€§èƒ½é«˜å‡ºå‡ ä¸ªæ•°é‡çº§
                count_sql = 'SELECT tagId, count(DISTINCT userId) AS user_count FROM "fxbackoffice_user_tags" GROUP BY tagId'
                counts_result = client.query(count_sql)
                counts_df = pd.DataFrame(counts_result.result_set, columns=counts_result.column_names)

                # ç¬¬ä¸‰æ­¥ï¼šæ•°æ®ç±»å‹è½¬æ¢ä¸åˆå¹¶ (ç¡®ä¿ tag_id åŒ¹é…)
                tags_df['tag_id'] = tags_df['tag_id'].astype(str)
                counts_df['tagId'] = counts_df['tagId'].astype(str)

                # ä½¿ç”¨ pandas è¿›è¡Œå·¦è¿æ¥ï¼Œç¡®ä¿å³ä½¿ç»„åˆ«ä¸‹æ²¡æœ‰ç”¨æˆ·ä¹Ÿèƒ½æ˜¾ç¤ºï¼ˆç”¨æˆ·æ•°ä¸º 0ï¼‰
                merged_df = pd.merge(tags_df, counts_df, left_on='tag_id', right_on='tagId', how='left')
                merged_df['user_count'] = merged_df['user_count'].fillna(0).astype(int)
                
                # æŒ‰ç…§ç”¨æˆ·æ•°é™åºæ’åºï¼Œæ–¹ä¾¿å‰ç«¯ä¼˜å…ˆå±•ç¤ºçƒ­é—¨ç»„åˆ«
                merged_df = merged_df.sort_values(by='user_count', ascending=False)
                
                group_list = merged_df[['tag_id', 'tag_name', 'user_count']].to_dict('records')
                
                # è®°å½•æ›´æ–°å†å²ï¼Œæ–¹ä¾¿å‰ç«¯å±•ç¤º
                previous_time = self._group_cache["last_update_time"] if self._group_cache else "N/A"
                
                result = {
                    "group_list": group_list,
                    "last_update_time": now.strftime('%Y-%m-%d %H:%M:%S'),
                    "previous_update_time": previous_time,
                    "total_groups": len(group_list)
                }

                # 4. æ›´æ–°å†…å­˜ç¼“å­˜åŠå…¶è¿‡æœŸæ—¶é—´
                self._group_cache = result
                self._cache_expiry = now + timedelta(days=7)
                
                logger.info(f"âœ… IB groups cache updated. Found {len(group_list)} groups.")
                return result

        except Exception as e:
            # è®°å½•è¯¦ç»†çš„å¼‚å¸¸å †æ ˆï¼Œæ–¹ä¾¿å°ç™½æ ¹æ®æ—¥å¿—å®šä½
            logger.error(f"âŒ Error fetching IB groups from ClickHouse: {str(e)}", exc_info=True)
            # å¦‚æœæŸ¥è¯¢å¤±è´¥ä½†ä¹‹å‰æœ‰æˆåŠŸåŠ è½½è¿‡çš„ç¼“å­˜ï¼Œåˆ™é™çº§è¿”å›æ—§ç¼“å­˜
            if self._group_cache:
                logger.warning("Using expired cache as fallback due to query error.")
                return self._group_cache
            # å¦åˆ™å‘ä¸ŠæŠ›å‡ºå¼‚å¸¸ï¼Œç”± API å±‚å¤„ç†
            raise e

    def get_pnl_analysis(
        self, 
        start_date: datetime, 
        end_date: datetime, 
        search: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Query ClickHouse for client PnL analysis within a date range.
        Returns a dict containing 'data' (list of records) and 'statistics' (query metadata).
        Includes Redis caching logic.
        """
        print(f"ğŸ” [ClickHouseService] Request: start={start_date}, end={end_date}, search={search}")
        
        # 1. ç”Ÿæˆç¼“å­˜ Key (åŸºäºæ—¥æœŸå’Œæœç´¢è¯è¿›è¡Œ MD5)
        search_key = (search or "").strip()
        cache_params = f"pnl_v1_{start_date.date()}_{end_date.date()}_{search_key}"
        cache_key = f"app:pnl:cache:{hashlib.md5(cache_params.encode()).hexdigest()}"

        # 2. å°è¯•ä» Redis è·å–ç¼“å­˜
        try:
            cached_data = self.redis_client.get(cache_key)
            if cached_data:
                print(f"ğŸš€ [Redis] Cache Hit: {cache_key}")
                res = json.loads(cached_data)
                # æ³¨å…¥ä»ç¼“å­˜è¯»å–çš„æ ‡è®°
                if "statistics" in res:
                    res["statistics"]["from_cache"] = True
                return res
        except Exception as re:
            print(f"âš ï¸ Redis Read Error: {re}")

        try:
            client = self.get_client()
            
            # æ ¼å¼åŒ–æ—¥æœŸå­—ç¬¦ä¸²
            # ç¡®ä¿ end_date åŒ…å«å½“å¤©çš„æœ€åä¸€ç§’ (ç”±è°ƒç”¨å±‚ä¼ å…¥ 23:59:59)
            start_str = start_date.strftime('%Y-%m-%d %H:%M:%S')
            end_str = end_date.strftime('%Y-%m-%d %H:%M:%S')
            
            # æ„å»º SQL
            sql = f"""
            WITH 
                ib_costs AS (
                    SELECT 
                        ticketSid, 
                        sum(commission) AS total_ib_cost
                    FROM fxbackoffice_ib_processed_tickets
                    WHERE close_time >= %(start_date)s 
                      AND close_time <= %(end_date)s
                    GROUP BY ticketSid
                )
            SELECT
                t.LOGIN AS account,
                m.userId AS client_id,     
                any(m.NAME) AS client_name,
                any(m.GROUP) AS group,
                -- Fresh grad note:
                -- `country` comes from fxbackoffice_users (CRM/BackOffice user profile).
                -- We convert empty string to NULL to keep "missing country" truly empty.
                any(NULLIF(u.country, '')) AS country,
                any(m.ZIPCODE) AS zipcode,
                any(m.CURRENCY) AS currency,
                any(m.sid) AS sid,
                any(u.partnerId) AS partner_id,
                any(ib_sum.net_deposit_usd) AS ib_net_deposit,
                'MT4' AS server,            
                
                countIf(t.CMD IN (0, 1)) AS total_trades,
                
                sumIf(t.lots, t.CMD IN (0, 1)) / if(any(m.CURRENCY) = 'CEN', 100, 1) AS total_volume_lots,
                
                sumIf(t.PROFIT, t.CMD IN (0, 1)) / if(any(m.CURRENCY) = 'CEN', 100, 1) AS trade_profit_usd,
                
                sumIf(t.SWAPS, t.CMD IN (0, 1)) / if(any(m.CURRENCY) = 'CEN', 100, 1) AS swap_usd,
                sumIf(t.COMMISSION, t.CMD IN (0, 1)) / if(any(m.CURRENCY) = 'CEN', 100, 1) AS commission_usd,
                
                COALESCE(sum(ib.total_ib_cost), 0) AS ib_commission_usd,
                
                ((sumIf(t.PROFIT + t.SWAPS + t.COMMISSION, t.CMD IN (0, 1)) * -1) / if(any(m.CURRENCY) = 'CEN', 100, 1)) - COALESCE(sum(ib.total_ib_cost), 0) AS broker_net_revenue,

                sumIf(t.PROFIT, t.CMD = 6) / if(any(m.CURRENCY) = 'CEN', 100, 1) AS period_net_deposit

            FROM fxbackoffice_mt4_trades AS t
            -- ä½¿ç”¨ INNER JOIN ç¡®ä¿åªæ˜¾ç¤ºå…³è”åˆ°æœ‰æ•ˆç”¨æˆ·çš„äº¤æ˜“
            INNER JOIN fxbackoffice_mt4_users AS m ON t.LOGIN = m.LOGIN
            LEFT JOIN fxbackoffice_users AS u ON m.userId = u.id
            LEFT JOIN ib_costs AS ib ON t.ticketSid = ib.ticketSid
            -- ä¿®æ”¹ï¼šä½¿ç”¨æ–°çš„ IB æ——ä¸‹å…¨é‡å‡€å…¥é‡‘æ±‡æ€»è¡¨ (sumMerge è¿˜åŸèšåˆçŠ¶æ€)
            LEFT JOIN (
                SELECT 
                    ibId, 
                    sumMerge(net_deposit) AS net_deposit_usd
                FROM ib_downline_net_deposit_agg
                GROUP BY ibId
            ) AS ib_sum ON toString(u.partnerId) = toString(ib_sum.ibId)
            WHERE 
                t.CLOSE_TIME >= %(start_date)s 
                AND t.CLOSE_TIME <= %(end_date)s
                AND t.CMD IN (0, 1, 6)
                
                AND m.userId > 0                   
                AND COALESCE(u.isEmployee, 0) != 1 
            """
            
            parameters = {
                'start_date': start_str,
                'end_date': end_str
            }

            if search:
                # æ€§èƒ½ä¼˜åŒ–ï¼šä»…æ”¯æŒ ClientID æˆ– AccountID æœç´¢ (å‰ç¼€åŒ¹é…)
                clean_search = search.strip()
                if clean_search:
                    sql += " AND (toString(m.userId) LIKE %(search)s OR toString(t.LOGIN) LIKE %(search)s)"
                    parameters['search'] = f"{clean_search}%"

            sql += """
            GROUP BY t.LOGIN, m.userId
            -- è¿‡æ»¤æ‰æ—¢æ²¡æœ‰äº¤æ˜“é‡ä¹Ÿæ²¡æœ‰å…¥é‡‘çš„è®°å½•
            HAVING total_volume_lots > 0 OR period_net_deposit != 0
            ORDER BY ib_commission_usd DESC
            """

            print(f"ğŸ“ [ClickHouseService] Execute SQL Params: {parameters}")
            
            # ä½¿ç”¨ client.query è·å–åŒ…å« summary çš„ç»“æœ
            result = client.query(sql, parameters=parameters)
            
            # è·å–åˆ—å
            columns = result.column_names
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            data = [dict(zip(columns, row)) for row in result.result_set]
            
            # å®‰å…¨è·å– elapsed æ—¶é—´ (ä¼˜å…ˆä½¿ç”¨çº³ç§’)
            elapsed_ns = result.summary.get('elapsed_ns', 0)
            if elapsed_ns:
                elapsed_seconds = float(elapsed_ns) / 1_000_000_000
            else:
                # Fallback: æŸäº›æ—§ç‰ˆæœ¬é©±åŠ¨å¯èƒ½åªæœ‰ elapsed (ç§’)
                elapsed_seconds = result.summary.get('elapsed', 0)

            # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œç›´æ¥è¿”å›
            if not data:
                return {
                    "data": [], 
                    "statistics": {
                        "elapsed": elapsed_seconds,
                        "rows_read": result.summary.get('read_rows', 0),
                        "bytes_read": result.summary.get('read_bytes', 0)
                    }
                }

            # ä½¿ç”¨ Pandas å¤„ç†æ•°æ®æ¸…æ´— (fillna ç­‰)
            df = pd.DataFrame(data)
            # IMPORTANT (fresh grad note):
            # Do NOT do `df.fillna(0)` on the whole DataFrame.
            # Reason: it will convert NULLs in TEXT columns into 0 (e.g. country/client_name/group),
            # which pollutes UI display and breaks "empty should stay NULL" requirements.
            #
            # Instead, we maintain a NUMERIC column whitelist and only fill NULLs for those metrics.
            # Maintenance rule:
            # - If you add/remove numeric metrics in the SELECT list (sum/count/amount fields),
            #   update this list accordingly.
            # - Keep dimension / identity fields (ids, names, group, zipcode, currency, server, country)
            #   OUT of this whitelist so they can remain NULL/empty naturally.
            #
            # Why a whitelist (instead of pandas dtype auto-detection)?
            # - In real datasets, numeric columns may arrive as strings/Decimal/object due to driver quirks,
            #   mixed types, or missing values. dtype-based detection can silently miss columns.
            # - A whitelist is explicit, stable, and aligned with business semantics.
            NUMERIC_FILLNA_ZERO_COLUMNS = [
                # Counts / volumes
                "total_trades",
                "total_volume_lots",
                # PnL / money metrics
                "trade_profit_usd",
                "swap_usd",
                "commission_usd",
                "ib_commission_usd",
                "broker_net_revenue",
                "period_net_deposit",
                # Joined metric (may come as string/None depending on source)
                "ib_net_deposit",
            ]

            # Normalize numeric columns and fill missing values with 0.
            # We use `to_numeric(..., errors='coerce')` to safely handle strings like "123.45".
            for col in NUMERIC_FILLNA_ZERO_COLUMNS:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors="coerce").fillna(0)
            
            processed_data = df.to_dict('records')

            result_dict = {
                "data": processed_data,
                "statistics": {
                    "elapsed": elapsed_seconds,
                    "rows_read": result.summary.get('read_rows', 0),
                    "bytes_read": result.summary.get('read_bytes', 0),
                    "from_cache": False
                }
            }

            # 3. å°†ç»“æœå­˜å…¥ Redis ç¼“å­˜ (è®¾ç½® TTL ä¸º 1800 ç§’ = 30 åˆ†é’Ÿ)
            try:
                self.redis_client.setex(
                    cache_key,
                    1800,
                    json.dumps(result_dict)
                )
                print(f"âœ… [Redis] Cache Saved: {cache_key}")
            except Exception as se:
                print(f"âš ï¸ Redis Save Error: {se}")

            return result_dict
            
        except Exception as e:
            print(f"ClickHouse Query Error: {e}")
            import traceback
            traceback.print_exc()
            # æŠ›å‡ºå¼‚å¸¸ä¾›ä¸Šå±‚å¤„ç†ï¼Œä¸å†åæ‰é”™è¯¯
            raise e

    def get_ib_report_data(
        self,
        r_start: datetime,
        r_end: datetime,
        m_start: datetime,
        m_end: datetime,
        target_groups: List[str]
    ) -> List[Dict[str, Any]]:
        """
        è·å– IB æŠ¥è¡¨æ•°æ®ã€‚
        æ‰§è¡Œå¤æ‚çš„èšåˆæŸ¥è¯¢ï¼Œè®¡ç®— Range å’Œ Month ç»´åº¦çš„èµ„é‡‘ã€äº¤æ˜“é‡å’Œä½£é‡‘æ•°æ®ã€‚
        
        ä¼˜åŒ–ï¼šå¢åŠ äº† Redis ç¼“å­˜æœºåˆ¶ï¼Œé¿å…é«˜å¹¶å‘ä¸‹é‡å¤æŸ¥è¯¢ã€‚
        """
        # 1. ç”Ÿæˆ Cache Key (è€ƒè™‘æ‰€æœ‰å½±å“ SQL çš„å˜é‡)
        try:
            # å°†ç»„åˆ«åˆ—è¡¨æ’åºï¼Œç¡®ä¿ ['A', 'B'] å’Œ ['B', 'A'] ç”Ÿæˆç›¸åŒçš„ key
            sorted_groups = sorted(target_groups or [])
            # ç»„åˆæ‰€æœ‰å‚æ•°ç”Ÿæˆå”¯ä¸€å­—ç¬¦ä¸²
            cache_params = f"ib_report_v1_{r_start}_{r_end}_{m_start}_{m_end}_{sorted_groups}"
            # ä½¿ç”¨ MD5 ç”ŸæˆçŸ­ key
            cache_key = f"app:ib_report:cache:{hashlib.md5(cache_params.encode()).hexdigest()}"
            
            # 2. å°è¯•ä» Redis è¯»å–
            if self.redis_client:
                cached_data = self.redis_client.get(cache_key)
                if cached_data:
                    logger.info(f"ğŸš€ [Redis] IB Report Cache Hit: {cache_key}")
                    return json.loads(cached_data)
        except Exception as e:
            logger.warning(f"âš ï¸ Redis Read Error: {e}")

        try:
            logger.info(f"ğŸ” Fetching IB report data from ClickHouse: range={r_start}~{r_end}, month={m_start}~{m_end}, groups={len(target_groups)}")
            
            with self.get_client(use_prod=True) as client:
                sql = """
                WITH
                    -- [1] å‚æ•°å®šä¹‰ (ç”±åç«¯åŠ¨æ€æ³¨å…¥)
                    toDateTime64(%(r_start)s, 6) AS r_start,
                    toDateTime64(%(r_end)s, 6) AS r_end,
                    toDateTime64(%(m_start)s, 6) AS m_start,
                    toDateTime64(%(m_end)s, 6) AS m_end,
                    toDate32(%(r_start)s) AS r_date_start,
                    toDate32(%(r_end)s) AS r_date_end,
                    toDate32(%(m_start)s) AS m_date_start,
                    toDate32(%(m_end)s) AS m_date_end,
                    %(target_groups)s AS target_groups,

                    -- [2] ç»„åˆ«æ˜ å°„: æ‰¾åˆ°ç›®æ ‡ç»„åˆ«ä¸‹çš„æ‰€æœ‰ User ID
                    group_mapping AS (
                        SELECT
                            t.tag AS group_name,
                            ut.userId AS user_id
                        FROM "fxbackoffice_tags" t
                        JOIN "fxbackoffice_user_tags" ut ON t.id = ut.tagId
                        WHERE t.categoryId = 6
                          AND (length(target_groups) = 0 OR has(arrayMap(x -> lower(x), target_groups), lower(t.tag)))
                    ),

                    -- [3] èµ„é‡‘ç»Ÿè®¡: Transactions è¡¨
                    money_stats AS (
                        SELECT
                            gm.group_name,
                            -- Range Stats
                            sumIf(if(upper(tr.processedCurrency) = 'CEN', tr.processedAmount / 100, tr.processedAmount), tr.type = 'deposit' AND tr.processedAt BETWEEN r_start AND r_end) AS deposit_range,
                            sumIf(if(upper(tr.processedCurrency) = 'CEN', tr.processedAmount / 100, tr.processedAmount), tr.type = 'withdrawal' AND tr.processedAt BETWEEN r_start AND r_end) AS withdrawal_range,
                            sumIf(if(upper(tr.processedCurrency) = 'CEN', tr.processedAmount / 100, tr.processedAmount), tr.type = 'ib withdrawal' AND tr.processedAt BETWEEN r_start AND r_end) AS ib_withdrawal_range,
                            -- Month Stats
                            sumIf(if(upper(tr.processedCurrency) = 'CEN', tr.processedAmount / 100, tr.processedAmount), tr.type = 'deposit' AND tr.processedAt BETWEEN m_start AND m_end) AS deposit_month,
                            sumIf(if(upper(tr.processedCurrency) = 'CEN', tr.processedAmount / 100, tr.processedAmount), tr.type = 'withdrawal' AND tr.processedAt BETWEEN m_start AND m_end) AS withdrawal_month,
                            sumIf(if(upper(tr.processedCurrency) = 'CEN', tr.processedAmount / 100, tr.processedAmount), tr.type = 'ib withdrawal' AND tr.processedAt BETWEEN m_start AND m_end) AS ib_withdrawal_month
                        FROM "fxbackoffice_transactions" tr
                        INNER JOIN group_mapping gm ON tr.fromUserId = gm.user_id
                        WHERE tr.status = 'approved' 
                          AND tr.type IN ('deposit', 'withdrawal', 'ib withdrawal')
                          AND tr.processedAt >= m_start
                        GROUP BY gm.group_name
                    ),

                    -- [4] äº¤æ˜“ç»Ÿè®¡: MT4 Trades è¡¨
                    trade_stats AS (
                        SELECT
                            gm.group_name,
                            -- Range Stats
                            sumIf(t.lots / if(mu.CURRENCY = 'CEN', 100, 1), t.CLOSE_TIME BETWEEN r_start AND r_end) AS volume_range,
                            sumIf((t.PROFIT + t.SWAPS + t.COMMISSION) / if(mu.CURRENCY = 'CEN', 100, 1), t.CLOSE_TIME BETWEEN r_start AND r_end) AS net_profit_range,
                            sumIf(t.COMMISSION / if(mu.CURRENCY = 'CEN', 100, 1), t.CLOSE_TIME BETWEEN r_start AND r_end) AS commission_range,
                            sumIf(t.SWAPS / if(mu.CURRENCY = 'CEN', 100, 1), t.CLOSE_TIME BETWEEN r_start AND r_end) AS swap_range,
                            -- Month Stats
                            sumIf(t.lots / if(mu.CURRENCY = 'CEN', 100, 1), t.CLOSE_TIME BETWEEN m_start AND m_end) AS volume_month,
                            sumIf((t.PROFIT + t.SWAPS + t.COMMISSION) / if(mu.CURRENCY = 'CEN', 100, 1), t.CLOSE_TIME BETWEEN m_start AND m_end) AS net_profit_month,
                            sumIf(t.COMMISSION / if(mu.CURRENCY = 'CEN', 100, 1), t.CLOSE_TIME BETWEEN m_start AND m_end) AS commission_month,
                            sumIf(t.SWAPS / if(mu.CURRENCY = 'CEN', 100, 1), t.CLOSE_TIME BETWEEN m_start AND m_end) AS swap_month
                        FROM "fxbackoffice_mt4_trades" t
                        INNER JOIN "fxbackoffice_mt4_users" mu ON t.LOGIN = mu.LOGIN
                        INNER JOIN group_mapping gm ON mu.userId = gm.user_id
                        WHERE t.CMD IN (0, 1) AND t.CLOSE_TIME >= m_start
                        GROUP BY gm.group_name
                    ),

                    -- [5] IBä½£é‡‘ç»Ÿè®¡: Stats é¢„èšåˆè¡¨
                    ib_commission_stats AS (
                        SELECT
                            gm.group_name,
                            -- Range Stats
                            sumIf(st.commission / if(upper(st.currency) = 'CEN', 100, 1), st.date BETWEEN r_date_start AND r_date_end) AS ib_commission_range,
                            -- Month Stats
                            sumIf(st.commission / if(upper(st.currency) = 'CEN', 100, 1), st.date BETWEEN m_date_start AND m_date_end) AS ib_commission_month
                        FROM "fxbackoffice_stats_ib_commissions_by_login_sid" st
                        -- Fix: Split SID-LOGIN format (e.g., '1-8522845') and match with mu.LOGIN
                        INNER JOIN "fxbackoffice_mt4_users" mu 
                            ON splitByChar('-', st.fromLoginSid)[2] = toString(mu.LOGIN)
                        INNER JOIN group_mapping gm ON mu.userId = gm.user_id
                        WHERE st.date >= m_date_start
                        GROUP BY gm.group_name
                    )

                -- [6] æœ€ç»ˆè¾“å‡º (Result Set)
                SELECT
                    coalesce(m.group_name, t.group_name, i.group_name) AS group,
                    
                    round(coalesce(m.deposit_range, 0), 2) AS deposit_range,
                    round(coalesce(m.deposit_month, 0), 2) AS deposit_month,
                    
                    round(coalesce(m.withdrawal_range, 0), 2) AS withdrawal_range,
                    round(coalesce(m.withdrawal_month, 0), 2) AS withdrawal_month,
                    
                    round(coalesce(m.ib_withdrawal_range, 0), 2) AS ib_withdrawal_range,
                    round(coalesce(m.ib_withdrawal_month, 0), 2) AS ib_withdrawal_month,
                    
                    -- Net Deposit = D + W + IBW (Arithmetic Sum)
                    round(coalesce(m.deposit_range, 0) + coalesce(m.withdrawal_range, 0) + coalesce(m.ib_withdrawal_range, 0), 2) AS net_deposit_range,
                    round(coalesce(m.deposit_month, 0) + coalesce(m.withdrawal_month, 0) + coalesce(m.ib_withdrawal_month, 0), 2) AS net_deposit_month,
                    
                    round(coalesce(t.volume_range, 0), 2) AS volume_range,
                    round(coalesce(t.volume_month, 0), 2) AS volume_month,
                    
                    round(coalesce(t.net_profit_range, 0), 2) AS profit_range,
                    round(coalesce(t.net_profit_month, 0), 2) AS profit_month,
                    
                    round(coalesce(t.commission_range, 0), 2) AS commission_range,
                    round(coalesce(t.commission_month, 0), 2) AS commission_month,

                    round(coalesce(t.swap_range, 0), 2) AS swap_range,
                    round(coalesce(t.swap_month, 0), 2) AS swap_month,
                    
                    round(coalesce(i.ib_commission_range, 0), 2) AS ib_commission_range,
                    round(coalesce(i.ib_commission_month, 0), 2) AS ib_commission_month

                FROM money_stats m
                FULL OUTER JOIN trade_stats t ON m.group_name = t.group_name
                FULL OUTER JOIN ib_commission_stats i ON coalesce(m.group_name, t.group_name) = i.group_name
                ORDER BY deposit_range DESC
                """
                
                params = {
                    'r_start': r_start.strftime('%Y-%m-%d %H:%M:%S'),
                    'r_end': r_end.strftime('%Y-%m-%d %H:%M:%S'),
                    'm_start': m_start.strftime('%Y-%m-%d %H:%M:%S'),
                    'm_end': m_end.strftime('%Y-%m-%d %H:%M:%S'),
                    'target_groups': target_groups
                }
                
                result = client.query(sql, parameters=params)
                
                # Convert result to list of dicts
                columns = result.column_names
                data = [dict(zip(columns, row)) for row in result.result_set]
                
                # 3. å°†ç»“æœå†™å…¥ Redis ç¼“å­˜ (è®¾ç½® 10 åˆ†é’Ÿè¿‡æœŸ)
                try:
                    if self.redis_client and data:
                        self.redis_client.setex(
                            cache_key,
                            600, # TTL 10 åˆ†é’Ÿ
                            json.dumps(data)
                        )
                        logger.info(f"âœ… [Redis] IB Report Cache Saved: {cache_key}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Redis Save Error: {e}")
                
                return data
                
        except Exception as e:
            logger.error(f"âŒ Error fetching IB report data: {str(e)}", exc_info=True)
            raise e

clickhouse_service = ClickHouseService()
